{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Doris-QZ/Reproducing-BERT-from-Scratch-with-PyTorch/blob/main/1_Data_For_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction\n",
        "\n",
        "This notebook prepares the **IMDB dataset** for BERT-style pre-training. The data is processed to be **compatible with BERT’s training objectives**, including:\n",
        "\n",
        "* **Masked Language Modeling (MLM)**: randomly masks tokens for prediction.\n",
        "\n",
        "* **Next Sentence Prediction (NSP)**: creates paired sentences with both positive and negative examples.\n",
        "\n",
        "Although this is not the same corpus used in the original BERT paper (BooksCorpus + Wikipedia), the dataset is structured in a way that allows the model to be trained with the same objectives.\n"
      ],
      "metadata": {
        "id": "EE8tNqYC23Xm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import random"
      ],
      "metadata": {
        "id": "xKa-gL4Z4_q4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Suppress the warning\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "\n",
        "# Load the data\n",
        "imdb_data = load_dataset('imdb', split='train')"
      ],
      "metadata": {
        "id": "kQwTagUg6gvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the data\n",
        "print(f\"{imdb_data}\\n\")\n",
        "imdb_data[0]"
      ],
      "metadata": {
        "id": "P1hngl8m6HrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build the dictionary\n",
        "\n",
        "First, we'll build a vocabulary that maps tokens---including special tokens---to their corresponding indices. Note that the BERT paper uses **WordPiece (a sub-word tokenization algorithm)** with a vocabulary of 30,000 token. Here, however, we use **NLTK’s word-based tokenizer**, which results in a much larger vocabulary."
      ],
      "metadata": {
        "id": "qWgSMxei1ZQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocab(dataset, special_tokens):\n",
        "    \"\"\"\n",
        "    Build a vocabulary mapping tokens to indices.\n",
        "\n",
        "    Args:\n",
        "        dataset (Dataset):The dataset from which to build the vocabulary.\n",
        "        special_tokens (List): A list of special tokens to include in the vocabylary.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary that maps tokens to their corresponding indices.\n",
        "\n",
        "    \"\"\"\n",
        "    # Add special tokens to the dictionary\n",
        "    num_special_tokens = len(special_tokens)\n",
        "    token_to_idx = {}\n",
        "\n",
        "    for i, token in enumerate(special_tokens):\n",
        "        token_to_idx[token] = i\n",
        "\n",
        "    # Initialize Counter object to track word frequencies\n",
        "    word_freq = Counter()\n",
        "\n",
        "    # Iterate through all datasets' texts and calculate word frequencies\n",
        "    for i in range(len(dataset)):\n",
        "        word_freq.update(word_tokenize(dataset[i]))\n",
        "\n",
        "    # Sort words by frequency and keep the top max_vocab_size words\n",
        "    sorted_words = sorted(word_freq.items(), key=lambda items: items[1], reverse=True)\n",
        "    vocab = [item[0] for item in sorted_words]\n",
        "\n",
        "    # Add words to token_to_idx dictionary\n",
        "    for idx, word in enumerate(vocab, start=len(special_tokens)):\n",
        "        token_to_idx[word] = idx\n",
        "\n",
        "    return token_to_idx\n",
        "\n",
        "# Build the vocabulary from the dataset\n",
        "vocab = build_vocab(dataset=imdb_data['text'],\n",
        "                    special_tokens=['[PAD]','[CLS]', '[SEP]','[MASK]','[UNK]']\n",
        "                    )\n",
        "\n",
        "len(vocab)"
      ],
      "metadata": {
        "id": "w_3Xl_ds2RCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function for Preparing Data for Masked Language Modeling (MLM)\n",
        "According to the BERT paper, **15% of tokens are randomly selected for masking**.  \n",
        "\n",
        "For example,<br>\n",
        "\n",
        "> Original text: \"my dog is very cute.\"  \n",
        "> Selected token: \"cute\"\n",
        "\n",
        "For the selected tokens:\n",
        "* 80% of time ---> replaced with [MASK] ---> \"my dog is very **[MASK]**.\"\n",
        "* 10% of time ---> left unchanged ---> \"my dog is very **cute**.\"\n",
        "* 10% of time ---> replaced with a random token ---> \"my dog is very **apple**.\"\n",
        "<br>\n",
        "\n",
        "Regardless of how the selected tokens are masked, the label remains the same:\n",
        ">[PAD][PAD][PAD][PAD] cute\n",
        "\n",
        "During training, the model **ignores the [PAD] tokens** and predicts the orginal tokens at the selected positions."
      ],
      "metadata": {
        "id": "5SyaYdlqLoX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def masking(token, vocab):\n",
        "    \"\"\"\n",
        "    Helper function to mask a single token for Masked Language Model (MLM) training.\n",
        "\n",
        "    Args:\n",
        "        token (str): The token to be processed.\n",
        "        vocab (dict): The vocabulary to choose random tokens from.\n",
        "\n",
        "    Returns:\n",
        "        tuple of two str---\n",
        "            1. The processed token, which may be replaced with '[MASK]', left unchanged, or replaced with a random token.\n",
        "            2. The label for the token--the original token if masked, or '[PAD]' if not masked.\n",
        "\n",
        "    \"\"\"\n",
        "    # The probability of a token being masked is 15%.\n",
        "    mask = random.random() <= 0.15\n",
        "\n",
        "    if not mask:\n",
        "        token_ = token\n",
        "        label_ = '[PAD]'\n",
        "        return token_, label_\n",
        "\n",
        "    # Generates a random float between 0 and 1\n",
        "    random_float = random.random()\n",
        "\n",
        "    # 80% of the selected tokens will be repalced by [MASK] token\n",
        "    if random_float < 0.8:\n",
        "        token_ = '[MASK]'\n",
        "        label_ = token\n",
        "        return token_, label_\n",
        "\n",
        "    # 10% of the selected tokens will remain unchanged\n",
        "    if random_float > 0.9:\n",
        "        token_ = token\n",
        "        label_ = token\n",
        "        return token_, label_\n",
        "\n",
        "    # 10% of the selected tokens will be replaced by a random token\n",
        "    else:\n",
        "        random_idx = random.randint(0, len(vocab) - 1)\n",
        "        token_ = list(vocab.keys())[random_idx]\n",
        "        label_ = token\n",
        "        return token_, label_"
      ],
      "metadata": {
        "id": "t5_R9cbvLVXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_for_MLM(dataset,  vocab):\n",
        "    \"\"\"\n",
        "    Prepare data for Masked Language model (MLM) training.\n",
        "\n",
        "    Args:\n",
        "        dataset (Dataset): The dataset to be processed.\n",
        "        vocab (dict): Vocabulary built from the dataset.\n",
        "\n",
        "    Returns:\n",
        "        tuple of two lists--\n",
        "            1. List of tokenized sentences, where each token is either replaced with '[MASK]', left unchanged, or replaced with a random token.\n",
        "            2. List of labels for masked tokens corresponding to the tokenized sentences. Each label is either the original token\n",
        "        at masked positions or [PAD] at unmasked positions.\n",
        "    \"\"\"\n",
        "\n",
        "    masked_sentences = []\n",
        "    labels = []\n",
        "    cur_tokens = []\n",
        "    cur_labels = []\n",
        "\n",
        "\n",
        "    for data in dataset:\n",
        "        tokens =  word_tokenize(data['text'])\n",
        "\n",
        "        for token in tokens:\n",
        "            token_, label_ = masking(token, vocab)\n",
        "            cur_tokens.append(token_)\n",
        "            cur_labels.append(label_)\n",
        "\n",
        "            # Found a token indicates the end of sentence, process the sentence and reset it.\n",
        "            if token in ['.', '?', '!']:\n",
        "                if len(cur_tokens) > 2:\n",
        "                    masked_sentences.append(cur_tokens)\n",
        "                    labels.append(cur_labels)\n",
        "                    cur_tokens = []\n",
        "                    cur_labels = []\n",
        "                else:\n",
        "                    cur_tokens = []\n",
        "                    cur_labels = []\n",
        "\n",
        "        # Note: the remaining tokens in the dataset that doesn't have an ending punctuation are ignored here. We can append it to the end of the list if we want.\n",
        "\n",
        "    return masked_sentences, labels\n"
      ],
      "metadata": {
        "id": "IeN1Rf9XFwWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function for Preparing Data for Next Sentence Prediction (NSP)\n",
        "\n",
        "BERT is trained to predict whether a pair of sentences are consecutive in the original text (**Next Sentence Prediction**).\n",
        "\n",
        "* **Positive examples**: the second sentence follows the first sentence in the dataset.\n",
        "\n",
        "* **Negative examples**: the second sentence is randomly selected from the dataset.\n",
        "\n",
        "For each sentence pair, a label is created:\n",
        "\n",
        "* 1 → Next sentence is correct (positive example)\n",
        "\n",
        "* 0 → Next sentence is incorrect (negative example)\n",
        "\n",
        "This allows the model to learn relationships between sentences in addition to word-level predictions."
      ],
      "metadata": {
        "id": "FFDpmLKYFbCs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_for_NSP(masked_sentences, labels):\n",
        "    \"\"\"\n",
        "    Prepare data for Next Sentence Prediction (NSP).\n",
        "\n",
        "    Args:\n",
        "        masked_sentences (List): List of tokenized sentences\n",
        "        labels (List): List of labels corresponding to input_tokens.\n",
        "\n",
        "    Returns:\n",
        "        tuple of three lists---\n",
        "            1. List of paired sentences with special tokens added.\n",
        "            2. List of labels for masked token.\n",
        "            3. List of boolean values for Next Sentence Prediction.\n",
        "    \"\"\"\n",
        "\n",
        "    # Make sure the length of inputs are valid\n",
        "    num_sentence = len(masked_sentences)\n",
        "    if num_sentence < 2:\n",
        "        raise ValueError(\"Must be more than two sentences in the input_tokens.\")\n",
        "\n",
        "    if num_sentence != len(labels):\n",
        "        raise ValueError(\"The input_tokens and the labels must have the same length.\")\n",
        "\n",
        "\n",
        "    paired_inputs = []\n",
        "    paired_labels = []\n",
        "    is_next = []\n",
        "\n",
        "    # Create the list of sentence indices\n",
        "    sentence_idx = list(range(num_sentence))\n",
        "\n",
        "    while len(sentence_idx) >= 2:\n",
        "        if random.random() >= 0.5:\n",
        "            # Randomly choose an index from the sentence_idx\n",
        "            idx = random.choice(sentence_idx[:-1])\n",
        "\n",
        "            # Pair two consecutive sentences (idx, idx+1) with special tokens as current inputs/lables\n",
        "            cur_input = [['[CLS]'] + masked_sentences[idx] + ['[SEP]'],\n",
        "                          masked_sentences[idx + 1] + ['[SEP]']]\n",
        "            cur_label = [['[PAD]'] + labels[idx] + ['[PAD]'],\n",
        "                          labels[idx + 1] + ['[PAD]']]\n",
        "\n",
        "            # Add current inputs/labels to paired_inputs/paired_labels\n",
        "            paired_inputs.append(cur_input)\n",
        "            paired_labels.append(cur_label)\n",
        "\n",
        "            # Append 1 to is_next, indicating the current input are consecutive sentences.\n",
        "            is_next.append(1)\n",
        "\n",
        "            # Remove idx and idx+1 from sentence_idx\n",
        "            sentence_idx.remove(idx)\n",
        "            if idx + 1 in sentence_idx:\n",
        "                sentence_idx.remove(idx+1)\n",
        "\n",
        "        else:\n",
        "            # Randomly sample two indices from the sentence_idx\n",
        "            idx_1, idx_2 = random.sample(sentence_idx, 2)\n",
        "\n",
        "            # Add two randomly selected sentences (idx_1, idx_2) with special tokens as current inputs/lables\n",
        "            cur_input = [['[CLS]'] + masked_sentences[idx_1] + ['[SEP]'],\n",
        "                         masked_sentences[idx_2] + ['[SEP]']]\n",
        "            cur_label = [['[PAD]'] + labels[idx_1] + ['[PAD]'],\n",
        "                         labels[idx_2] + ['[PAD]']]\n",
        "\n",
        "            # Add current inputs/labels to paired_inputs/paired_labels\n",
        "            paired_inputs.append(cur_input)\n",
        "            paired_labels.append(cur_label)\n",
        "\n",
        "            # Append 0 to is_next, indicating the current input are not consecutive sentences.\n",
        "            is_next.append(0)\n",
        "\n",
        "            # Remove idx_1 and idx_2 from sentence_idx\n",
        "            sentence_idx.remove(idx_1)\n",
        "            sentence_idx.remove(idx_2)\n",
        "\n",
        "    return paired_inputs, paired_labels, is_next"
      ],
      "metadata": {
        "id": "swNZTkrClkgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function to Create Final BERT Training Data\n",
        "\n",
        "The final function combines the **MLM** and **NSP** data preparation steps:\n",
        "\n",
        "1. Randomly masks 15% of tokens and prepares the corresponding labels for **Masked Language Modeling (MLM)**.\n",
        "\n",
        "2. Generates sentence pairs and labels for **Next Sentence Prediction (NSP)**.\n",
        "\n",
        "3. Returns a **pandas DataFrame** containing all the processed data.\n",
        "\n",
        "The output DataFrame can then be saved as a CSV file, which is later loaded in the `Reproducing BERT Model from Scratch using PyTorch.ipynb` notebook. In that notebook, the data is **converted to a PyTorch dataset** and loaded into a **PyTorch DataLoader** for training."
      ],
      "metadata": {
        "id": "jUBXZJdt1ULt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_for_BERT(dataset,  vocab):\n",
        "    \"\"\"\n",
        "    Prepare data for BERT training--Masked Language Modeling and Next Sentence Prediction.\n",
        "\n",
        "    Args:\n",
        "        dataset(Dataset): The dataset to be processed.\n",
        "        vocab (dict): The vocabulary built from the dataset.\n",
        "\n",
        "    Returns:\n",
        "        A Pandas dataframe\n",
        "\n",
        "    \"\"\"\n",
        "    # Pad the paired inputs and flatten the nested list\n",
        "    def pad_flatten(pairs, padding='[PAD]'):\n",
        "        max_len = max(len(pairs[0]), len([pairs[1]]))\n",
        "        pairs[0].extend([padding] * (max_len - len(pairs[0])))\n",
        "        pairs[1].extend([padding] * (max_len - len(pairs[1])))\n",
        "        flatten_pairs = [item for sublist in pairs for item in sublist]\n",
        "        return flatten_pairs\n",
        "\n",
        "    # Convert tokens to indices\n",
        "    token_to_idx = lambda tokens: [vocab[token] for token in tokens]\n",
        "\n",
        "    # Get masked_sentences and the corresponding labels from the dataset\n",
        "    masked_sentences, labels = data_for_MLM(dataset,  vocab)\n",
        "\n",
        "    # Get paired_sentences, labels, and is_next list\n",
        "    paired_sentences, labels, is_next = data_for_NSP(masked_sentences, labels)\n",
        "\n",
        "    bert_inputs, bert_labels, segment_labels = [], [], []\n",
        "\n",
        "    for sentences, labels in zip(paired_sentences, labels):\n",
        "        # Create and pad segment labels\n",
        "        seg_label = [[1] * len(sentences[0]), [2] * len(sentences[1])]\n",
        "        seg_label = pad_flatten(seg_label, padding=0)\n",
        "\n",
        "        # Pad and flatten paired sentences and labels\n",
        "        padded_sent = pad_flatten(sentences)\n",
        "        padded_label = pad_flatten(labels)\n",
        "\n",
        "        # Convert tokens to indices and add to final lists\n",
        "        bert_inputs.append(token_to_idx(padded_sent))\n",
        "        bert_labels.append(token_to_idx(padded_label))\n",
        "        segment_labels.append(seg_label)\n",
        "\n",
        "    # Create a dataframe of bert data\n",
        "    bert_data = pd.DataFrame({\n",
        "        'bert_inputs': bert_inputs,\n",
        "        'bert_labels': bert_labels,\n",
        "        'segment_labels': segment_labels,\n",
        "        'is_next': is_next\n",
        "    })\n",
        "\n",
        "    return bert_data"
      ],
      "metadata": {
        "id": "DLl-LunGw_z7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process imdb_data for BERT training\n",
        "imdb_bert_data = data_for_BERT(imdb_data,  vocab)\n",
        "imdb_bert_data.head()"
      ],
      "metadata": {
        "id": "_YqjQLFwtMPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imdb_bert_data.info()"
      ],
      "metadata": {
        "id": "T0ZcWDYXa0B9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the dataframe as CSV\n",
        "imdb_bert_data.to_csv('imdb_bert_data.csv', index=False)"
      ],
      "metadata": {
        "id": "QdP5h8tra6yJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zLfDNycMa2qC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}